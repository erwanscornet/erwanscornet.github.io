<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
	<title>Webpage of Erwan Scornet </title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<meta name="author" content="Erwan Scornet" />
	<meta name="description" content="Webpage of Erwan Scornet, professor of mathematics at LPSM, SCAI, Sorbonne Université)" />
	<meta name="keywords" content="Erwan Scornet, machine learning, statistique, random forests, forêts aléatoires, arbres" />
	<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
</head>
<body>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header" class="alt">
			<!--<span class="logo"><img src="images/logo.svg" alt="" /></span>-->
			<h1>Erwan Scornet, professor at Sorbonne University </h1>
			<h2>Statistic, Machine Learning </h2>

			<ul class="icons">
				<li>
					

</li>
<li><a href="https://github.com/erwanscornet" class="icon fa-github alt"><span class="label">GitHub</span></a></li>
<li><a href="https://scholar.google.fr/citations?user=6Qt1NFoAAAAJ&hl=fr" class="icon fa-google alt"><span class="label">Google Scholar</span></a></li>
</ul>
</header>

<!-- Nav -->
<nav id="nav">
	<ul>
		<li><a href="#header">Top</a></li>
		<li><a href="#intro" class="active">Bio</a></li>
		<!--<li><a href="#first">News</a></li>-->
		<!--<li><a href="#second">Research</a></li>-->
		<li><a href="#students">Contact</a></li>
		<li><a href="#cta">Publications</a></li>
		<li><a href="#teach">Teaching</a></li>
		<li><a href="#talks">Talks</a></li>
		<li><a href="#footer">Contact</a></li>
	</ul>
</nav>

<!-- Main -->
<div id="main">

	<!-- Introduction -->
	<section id="intro" class="main">
		<div class="spotlight">
			<div class="content">
				<header class="major">
					<h2>Short Bio</h2>
				</header>
				<div align="justify">
				<p>Since september 2023, I am a professor (lecturer) at LPSM and SCAI in Sorbonne Université (previously known as Paris 6, in the center of Paris). Before that, I was an assistant professor at the Center for Applied Mathematics (CMAP) in Ecole Polytechnique near Paris. 
					My research interests focus on theoretical statistics and Machine Learning, 
					with a particular emphasis on nonparametric estimates. 
					I did my PhD thesis on a particular algorithm of Machine Learning called random forests, 
					under the supervision of <a href="http://www.lsta.upmc.fr/biau.html"> G&eacuterard Biau </a> (LSTA - Paris 6)
					and <a href="http://cbio.ensmp.fr/~jvert/">  Jean-Philipe Vert </a> (Institut Curie). </p>

					<b>Keywords:</b> statistical learning, non-parametric estimation, random forests, decision trees, variable importance, 
					missing data, neural networks, causal inference.


				</div> <!-- TODO -->
						<!-- <ul class="actions">
							<li><a href="" class="button">Curriculum Vit&aelig; (english)</a></li>
							<li><a href="" class="button">Curriculum Vit&aelig; (français)</a></li>
						</ul> -->
					</div>
					<span class="image"><img src="images/photo4.jpeg" alt="" /></span>
				</div>
			<h2> <a href="pdf/cv_scornet_compressed.pdf">  Curriculum Vitae  </a> </h2>
			</section>

			
			
			
			


			<!-- Third Section -->
			<section id="first" class="main special">
				<header class="major">
				<h2> Awards and distinctions </h2>
				</header>

				<ul>
				<li> Winner of the <a href="http://smai.emath.fr/spip.php?article359"> Jacques Neveu 2016 Prize </a>  for a thesis in the field of probability or statistic. 
				</ul>
			</section>

				<!--<header class="major">
					<h2>News</h2>
				</header>
				<ul class="features">-->
					<!-- <li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>Date</h3>
						<p><a href="#cta">Revised version of our preprint with </a> with <a href="http://math.univ-lille1.fr/~celisse/">Alain Celisse</a>, about stability and generalisation bounds for the Leave-one-Out.</p>
					</li> -->
					<!-- <li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>Date</h3>
						<p><a href="#cta">New preprint</a> with <a href="http://alquier.ensae.net">Pierre Alquier</a>, about xxx.</p>
					</li> -->
					
					<!--<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>December 9, 2017</h3>
						<p>I am running the <a href="http://nips.cc">NIPS 2017</a> workshop <em><a href="nips2017/50shadesbayesian.html">(Almost) 50 shades of Bayesian Learning: PAC-Bayesian trends and insights</a></em>.
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>November 8, 2017</h3>
						<p>Our paper with <a href="http://alquier.ensae.net">Pierre Alquier</a> on PAC-Bayesian bounds for hostile data has been accepted for publication in the <a href="http://www.springer.com/computer/ai/journal/10994">Machine Learning Journal</a>!
						</p>
					</li>
					<li>
						<span class="icon major style4 fa-graduation-cap"></span>
						<h3>November 1, 2017</h3>
						<p>Delighted to greet my new colleague and office mate <a href="http://chercheurs.lille.inria.fr/pgermain/">Pascal Germain</a>. Welcome! <a href="https://modal.lille.inria.fr/wikimodal/doku.php">Modal</a> is doubling its PAC-Bayes coverage!</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>October 28, 2017</h3>
						<p>Our paper with <a href="https://sites.google.com/site/sylvainrobbiano/">S. Robbiano</a> on PAC-Bayesian bounds for high-dimensional ranking has been accepted for publication in the <a href="https://www.journals.elsevier.com/journal-of-statistical-planning-and-inference/">Journal of Statistical Planning and Inference</a>!
						</p>
					</li>
					<li>
						<span class="icon major style4 fa-graduation-cap"></span>
						<h3>October 1, 2017</h3>
						<p>My new student Arthur Leroy has started his PhD. Welcome!</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>September 11, 2017</h3>
						<p>Our NIPS workshop <em><a href="nips2017/50shadesbayesian.html">(Almost) 50 shades of Bayesian Learning: PAC-Bayesian trends and insights</a></em> has been accepted by NIPS!
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>July 27, 2017</h3>
						<p>New version of our preprint and Python library <a href="https://github.com/bhargavvader/pycobra">pycobra</a> with Bhargav Srinivasa Desikan.
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>May 26, 2017</h3>
						<p>Our paper on quasi-Bayesian NMF (with <a href="http://alquier.ensae.net">Pierre Alquier</a>) is now published in <a href="http://www.springer.com/statistics/statistical+theory+and+methods/journal/12004"><em>Mathematical Methods of Statistics</em></a>!
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>April 26, 2017</h3>
						<p><a href="#cta">New preprint</a> with Bhargav Srinivasa Desikan on <a href="https://github.com/bhargavvader/pycobra">pycobra</a>, a Python library for ensemble regression analysis and visualisation.</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>April 7, 2017</h3>
						<p><a href="#cta">Revised version of our preprint</a> with Le Li and <a href="http://www.math.univ-angers.fr/~loustau/">Sébastien Loustau</a>, about quasi-Bayesian online clustering.</p>
					</li>
					<li>
						<span class="icon major style2 fa-code-fork"></span>
						<h3>March 27, 2017</h3>
						<p>New Python library released with Bhargav Srinivasa Desikan: <a href="https://github.com/bhargavvader/pycobra">pycobra</a> for ensemble regression analysis and visualisation.
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>February 1, 2017</h3>
						<p>Our paper on quasi-Bayesian NMF (with <a href="http://alquier.ensae.net">Pierre Alquier</a>) has been accepted for publication in <a href="http://www.springer.com/statistics/statistical+theory+and+methods/journal/12004"><em>Mathematical Methods of Statistics</em></a>!
						</p>
					</li>
					<li>
						<span class="icon major style4 fa-chevron-up"></span>
						<h3>January 1, 2017</h3>
						<p>I got promoted to a first class researcher (CR1) Inria position!</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>October 23, 2016</h3>
						<p><a href="#cta">New preprint</a> together with <a href="http://alquier.ensae.net">Pierre Alquier</a>, about PAC-Bayesian bounds for hostile data.</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>August 24, 2016</h3>
						<p><a href="#cta">Revised version of our preprint</a> with <a href="http://alquier.ensae.net">Pierre Alquier</a>, about quasi-Bayesian non-negative matrix factorization.</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>August 23, 2016</h3>
						<p><a href="#cta">New preprint</a> together with <a href="http://math.univ-lille1.fr/~celisse/">Alain Celisse</a>, about stability and generalisation bounds for the Leave-one-Out.</p>
					</li>
					<li>
						<span class="icon major style4 fa-graduation-cap"></span>
						<h3>August 22, 2016</h3>
						<p>My new student Bhargav Srinivasa Desikan has joined Inria. Welcome!</p>
					</li>
					<li>
						<span class="icon major style6 fa-cloud-upload"></span>
						<h3>July 27, 2016</h3>
						<p>New website!</p>
					</li>-->
					<!-- <li>
						<span class="icon major style1 fa-code"></span>
						<h3>Ipsum consequat</h3>
						<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
					</li>
					<li>
						<span class="icon major style3 fa-copy"></span>
						<h3>Amed sed feugiat</h3>
						<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
					</li> -->
				</ul>
				<!-- <footer class="major">
					<ul class="actions">
						<li><a href="generic.html" class="button">Learn More</a></li>
					</ul>
				</footer> -->
			</section>

			<!-- Second Section -->
			<section id="students" class="main">
				<!--<header class="major">
					<h2>Research</h2>
				</header>
				<p>My main line of research is in statistical machine learning. I am primarily interested in the design, analysis and implementation of statistical learning methods for high dimensional problems. My interests include (but are not limited to): PAC-Bayesian theory, sparsity and high-dimensional statistics, optimisation theory, statistical learning theory, non-negative matrix factorisation, aggregation of estimators and classifiers, MCMC algorithms, (un)supervised learning, online clustering, concentration inequalities...
				</p>
				<ul class="statistics">
					<li class="style1">
						<span class="icon fa-files-o"></span>
						<strong>11</strong> Research Articles <br> (7 published)
					</li>
					<li class="style3">
						<span class="icon fa-commenting-o"></span>
						<strong>48</strong> Talks since 2010
					</li>
					<li class="style4">
						<span class="icon fa-graduation-cap"></span>
						<strong>10</strong> Students <br> (2 Ph.D.)
					</li>
					<li class="style5">
						<span class="icon fa-building-o"></span>
						<strong>4</strong> Collaborations with companies
					</li>
				</ul>-->
				<h2>Students</h2>
				<ol>
					<li>
						<a href="http://www.cmapx.polytechnique.fr/~jaouad.mourtada/">Jaouad Mourtada</a> (2016-2020) <br>
						Ph.D. student co-supervised with <a href="http://www.cmap.polytechnique.fr/~gaiffas/">Stéphane Gaïffas</a> 
					</li>

					<li>
						<a href="https://nprost.github.io/"> Nicolas Prost </a>  (2018-2019) <br>
						Ph.D. student co-supervised with <a href="http://juliejosse.com/">Julie Josse</a> and
						<a href="http://gael-varoquaux.info/">Gaël Varoquaux </a> 
					</li>

					<li> 
						<a href="https://www.linkedin.com/in/clement-benard-308a267a/?originalSubdomain=fr"> Clément Bénard </a> (2018-2021) <br>
						Ph.D. student co-supervised with <a href="http://www.lsta.upmc.fr/biau.html">Gérard Biau </a> and
						<a href="https://www.linkedin.com/in/s%C3%A9bastien-da-veiga-80791013a/?originalSubdomain=fr">Sébastien Da Veiga </a> 
					</li>

					<li>
						<a href="https://ludovic-arnould.github.io/"> Ludovic Arnould </a> (2020-) <br>
						Ph.D. student co-supervised with <a href="http://www.lpsm.paris/pageperso/boyer/">Claire Boyer </a>  
					</li>

					<li>
						<a href="https://benedictecolnet.github.io/"> Bénédicte Colnet </a> (2020-2023) <br>
						Ph.D. student co-supervised with <a href="http://juliejosse.com/">Julie Josse</a> and
						<a href="http://gael-varoquaux.info/">Gaël Varoquaux </a> 
					</li>

					<li>
						<a href="https://alexisayme.github.io/"> Alexis Ayme </a> (2021-) <br>
						Ph.D. student co-supervised with <a href="http://www.lpsm.paris/pageperso/boyer/">Claire Boyer </a> and <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/">Aymeric Dieuleveut </a>   
					</li>
					
					<li>
						<a href="https://www.linkedin.com/in/abdoulaye-sakho-a48b571b6/?originalSubdomain=fr"> Abdoulaye Sakho </a> (2023-) <br>
						Ph.D. student (CIFRE at <a href="https://www.artefact.com/fr/"> Artefact</a>) co-supervised with <a href="https://www.linkedin.com/in/emmanuel-malherbe-0b60a440/?originalSubdomain=fr"> Emmanuel Malherbe </a>    
					</li>

					<li>
						<a href="https://www.linkedin.com/in/ahmed-boughdiri-3a127a169/?originalSubdomain=fr"> Ahmed Boughdiri </a> (2023-) <br>
						Ph.D. student co-supervised with <a href="http://juliejosse.com/">Julie Josse</a>   
					</li>
				</ol>
					<!-- <footer class="major">
						<ul class="actions">
							<li><a href="generic.html" class="button">Learn More</a></li>
						</ul>
					</footer> -->
				</section>

				<!-- Get Started -->
				<section id="cta" class="main">
					<header class="major">
						<h2>Publications</h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>
					<h3>Preprints</h3>
					<ol>
						<li> J. Josse, N. Prost, E. Scornet, G. Varoquaux. <a href="https://arxiv.org/pdf/1902.06931.pdf"> On the consistency of supervised learning with missing values </a>, 2019  </li>
						<li> B. Colnet, J. Josse, G. Varoquaux, E. Scornet. <a href="https://arxiv.org/abs/2208.07614"> Reweighting the RCT for generalization: finite sample analysis and variable selection </a>, 2022. </li>
						<li> B. Colnet, J. Josse, G. Varoquaux, E. Scornet. <a href="https://arxiv.org/abs/2303.16008"> Risk ratio, odds ratio, risk difference... Which causal measure is easier to generalize? </a>, 2023. </li>
						<li> A. Sakho, E. Malherbe, E. Scornet. <a href="https://arxiv.org/abs/2402.03819"> Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies </a>, 2024. </li>
						<li> A. Ayme, C. Boyer, A. Dieuleveut, E. Scornet. <a href="https://arxiv.org/abs/2402.03839"> Random features models: a way to study the success of naive imputation </a>, 2024. </li>

					</ol>
					
					<h3>Accepted/Published papers</h3>
					<ol>
						<li> Scornet, E., Biau, G. and Vert, J.-P. (2015). <a href="pdf/article.pdf">Consistency of random forests,</a> The Annals of Statistics, Vol. 43, pp. 1716-1741
							(<a href="pdf/supplementary_file.pdf">Supplementary materials </a>).
						<li> Scornet, E. (2016). <a href="pdf/article_asymptotics_RF">On the asymptotics of random forests,</a> Journal of Multivariate Analysis, Vol. 146, pp. 72-83.
						<li> Scornet, E. (2016). <a href="pdf/articlekernel.pdf"> Random forests and kernel methods,</a> IEEE Transactions on Information Theory, Vol. 62, pp. 1485-1500.
						<li> Biau, G., Scornet, E. (2016). <a href="pdf/reviewforest.pdf"> A Random Forest Guided Tour,</a> TEST, Vol. 25, pp. 197-227. (<a href="paper/test_rejoinder.pdf"> Discussion </a>).
						<li> Scornet, E. (2016). <a href="pdf/article_MATAPLI.pdf"> Promenade en for&ecircts al&eacuteatoires,</a> MATAPLI, Vol. 111. 
						<li> E. Bernard, Y. Jiao, E. Scornet, V. Stoven, T. Walter and J.-P. Vert (2017) <a href="http://www.biorxiv.org/content/early/2017/08/01/171298"> Kernel multitask regression for toxicogenetics,</a> Molecular Informatics, Vol. 36. 
						<li> J. Mourtada, S. Ga&iumlffas, E. Scornet, (2017) <a href="pdf/article_mondrian.pdf"> Universal consistency and minimax rates for online Mondrian Forest,</a> NIPS 2017 (<a href="paper/article_mondrian_supplementary.pdf">Supplementary materials </a>). 
						<li> Scornet, E. (2017). <a href="https://www.esaim-proc.org/articles/proc/abs/2017/05/contents/contents.html"> Tuning parameters in random forests,</a> ESAIM Procs, Vol. 60 pp. 144-162.</li>
						<li> R. Duroux, E. Scornet (2018) <a href="https://hal.archives-ouvertes.fr/hal-01287521"> Impact of subsampling and tree depth on random
							forests</a>, ESAIM: Probability and Statistics, Vol. 22, pp. 96-128. </li>
						<li> G. Biau, E. Scornet, J. Welbl, (2018) <a href="https://arxiv.org/abs/1604.07143"> Neural Random Forests </a>, Sankhya A, pp. 1-40.  </li>
						<li> J. Mourtada, S. Ga&iumlffas, E. Scornet (2020) <a href="https://arxiv.org/abs/1803.05784"> Minimax optimal rates for Mondrian trees and forests </a>, The Annals of Statistics, 48(4), 2253-2276. </li>
						<li> M. Le Morvan, N. Prost, J. Josse, E. Scornet. & G. Varoquaux (2020)  <a href="https://arxiv.org/abs/2002.00658"> Linear predictor on linearly-generated data with missing values: non consistency and solutions </a>, AISTAT. </li>
						<!-- pdf/article_aistat_2020.pdf --> 
						<li> M. Le Morvan, J. Josse, T. Moreau, E. Scornet, G. Varoquaux (2020) <a href="https://arxiv.org/abs/2007.01627"> Neumann networks: differential programming for supervised learning with missing values </a>, NeurIPS (oral communication). </li> 
						<li> C. B&eacute;nard, G. Biau, S. Da Veiga, E. Scornet (2021) <a href="https://arxiv.org/abs/1908.06852"> SIRUS: Stable and Interpretable RUle Set for Classification </a>, Electronic Journal of Statistics, Vol. 15, pp. 427-505.  </li>
						<li> C. B&eacute;nard, G. Biau, S. Da Veiga, E. Scornet (2021). <a href="https://arxiv.org/abs/2004.14841"> Interpretable Random Forests via Rule Extraction </a>, AISTAT.  </li>
						<li> J. Mourtada, S. Ga&iumlffas, E. Scornet (2021). <a href="https://arxiv.org/abs/1906.10529"> AMF: Aggregated Mondrian Forests for Online Learning </a>, Journal of the Royal Statistical Society: Series B (Statistical Methodology), 83(3), 505-533. </li> 
						<li> L. Arnould, C. Boyer, E. Scornet (2021). <a href="https://arxiv.org/abs/2010.15690"> Analyzing the tree-layer structure of Deep Forests </a>, ICML.  </li>
						<li> M. Le Morvan, J. Josse, E. Scornet, G. Varoquaux (2021). <a href="https://hal.archives-ouvertes.fr/hal-03243931">  What's a good imputation to predict with missing values? </a>, NeurIPS. </li>
						<li> E. Scornet (2021). <a href="https://arxiv.org/abs/2001.04295"> Trees, forests, and impurity-based variable importance </a>, <a href="https://imstat.org/journals-and-publications/annales-de-linstitut-henri-poincare/annales-de-linstitut-henri-poincare-accepted-papers/"> Annales de l’Institut Henri Poincaré </a> </li> 
						<li> C. B&eacute;nard, G. Biau, S. Da Veiga, E. Scornet (2022). <a href="https://arxiv.org/pdf/2105.11724.pdf">  SHAFF: Fast and consistent SHApley eFfect estimates via random Forests </a>, AISTAT. </li>
						<li> C. B&eacute;nard, S. Da Veiga, E. Scornet (2022). <a href="https://arxiv.org/abs/2102.13347"> MDA for random forests: inconsistency, and a practical
							solution via the Sobol-MDA </a>, Biometrika. </li>	
						<li> A. Ayme, C. Boyer, A. Dieuleveut, E. Scornet (2022). <a href="https://arxiv.org/abs/2202.01463"> Near-optimal rate of consistency for linear models with missing values </a>, ICML. </li>
						<li> B. Colnet, J. Josse, E. Scornet, G. Varoquaux (2022). <a href="https://arxiv.org/abs/2105.06435"> Generalizing a causal effect: sensitivity analysis and missing covariates </a>, accepted for publication in Journal of Causal Inference. </li>
						<li> L. Arnould, C. Boyer, E. Scornet (2023). <a href="https://arxiv.org/abs/2202.03688" > Is interpolation benign for regression random forests? </a>, AISTAT. </li>
						<li> P. Lutz, L. Arnould, C. Boyer, E. Scornet (2023). <a href="https://hal.archives-ouvertes.fr/hal-03792008/document"> Sparse tree-based initialization for neural networks </a>, ICLR. </li>
						<li> A. Ayme, C. Boyer, A. Dieuleveut, E. Scornet (2023) <a href="https://arxiv.org/abs/2301.13585"> Naive imputation implicitly regularizes high-dimensional linear models </a>, ICML. </li>				
					</ol>

					<h3>Book</h3>
					<ol>
						<li>  B. Iooss, R. Kenett, P. Secchi, B.M. Colosimo, F. Centofanti, C. Bénard, S. Da Veiga, E. Scornet, S. N. Wood, Y. Goude, M. Fasiolo  <a href="https://link.springer.com/book/10.1007/978-3-031-12402-0"> Interpretability for Industry 4.0 : Statistical and Machine Learning Approaches </a>, 
							Editors: A. Lepore, B. Palumbo, J.-M. Poggi, Springer 2022. </li>
					</ol>

					<h3>Academic publications</h3>
					<ol>
						 <li> PhD thesis <a href="pdf/these.pdf"> Learning with random forests</a>, defended on Monday, 30th November, 2015.   </li>
						 <li> HDR manuscript <a href="pdf/HDR_scornet.pdf"> Random forests, interpretability, neural networks and missing values</a>, defended on the 17th December, 2020.   </li>
					</ol>
					<!-- <footer class="major">
						<ul class="actions">
							<li><a href="generic.html" class="button special">Get Started</a></li>
							<li><a href="generic.html" class="button">Learn More</a></li>
						</ul>
					</footer> -->
				</section>


				<section id="teach" class="main">
					<header class="major">
						<h2>Teaching</h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>
					
					
					
					<ol>
						<li>  Decision Trees: <a href="teaching/DecisionTrees.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV-b1KL0D0Uax9GUQbxUBLdZ"> English </a> or in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV_eAfyrlwQyucM3RDqBeWpU"> French </a>
						<li>  Random Forests and Tree Boosting: <a href="teaching/RandomForests.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV_aHOsQn27Wnmya2e4eb24x"> English </a> or in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV-eBAtmDEkCA5M9iU9C1A6i"> French </a>
						<li>  Introduction to neural networks: <a href="teaching/RegularNN.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV8GDDgQNNWnVrozSlNZwv4R"> English </a> or in <a href="teaching/RegularNN.pdf"> French </a>
						<li>  Hyperparameter tuning in neural networks: <a href="teaching/NNHyperparameters.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV_SaCc5ssBRdDudjShWdnMe"> English </a> or in <a href="teaching/RegularNN.pdf"> French </a>
						<li>  Convolutional Neural Networks: <a href="teaching/CNN.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV_4AHk7dLlxJRcvPVSnuLCV"> English </a>
						<li>  Applications of Convolutional Neural Networks: <a href="teaching/CNN_applications.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV8-W7g1UizIZ9Cr5ZlREtla"> English </a>
						<li>  Recurrent Neural Networks: <a href="teaching/RNN.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV9C8tsTvwFxNBn-arFA2r4Z"> English </a>
						<li>  A detour through unsupervised learning: <a href="teaching/unsupervised_learning.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV-p2JIjjrlM8nPOPoMb0Vwo"> English </a>
						<li>  Generative Models: <a href="teaching/Generative_Models.pdf"> Slides </a> and lectures in <a href="https://www.youtube.com/playlist?list=PLqTFBD-cpDV-p2JIjjrlM8nPOPoMb0Vwo"> English </a>		
						<li>  Word Embedding <a href="teaching/Word_Embedding.pdf"> Some slides in construction </a>
					</ol>
					
				
				</section>

				<section id="cta" class="main">
					<header class="major">
						<h2>Statistics and Video games - Stone's theorem </h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>

					<ol> You need to download the corresponding game package (Mac or PC) and launch the .exe file (you may need to download <a href="https://www.renpy.org/"> RenPy </a>).
					<li>  <a href="Theoreme_Stone.pdf"> Stone's theorem (statement) </a>
					<li>  <a href="Theoreme_Stone-1.0-mac.zip"> Game package for Mac</a>
					<li>  <a href="Theoreme_Stone-1.0-pc.zip"> Game package for PC</a>
						<ol>

				</section>

				<section id="talks" class="main">
					<header class="major">
						<h2>Talks</h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>
					
					
					<ol>
						<li>  <a href="talks/random_forests.pdf"> Random Forests </a>
						<li> <a href="talks/Intro_to_AI.pdf"> General overview of AI </a>	
						<li> <a href="talks/AI_for_health.pdf"> Ai for health </a>	
						<li> <a href="talks/mdi.pdf"> Trees, forests, and impurity-based variable importance </a>			
						<li> <a href="talks/interpolation.pdf"> Is interpolation benign for random forest regression? </a> Paper <a href="https://arxiv.org/abs/2202.03688"> here </a>			
						<li> <a href="talks/StatLearn23.pdf"> Pour une botanomancie rigoureuse: lire l'importance dans les feuilles
							des for&ecirc;ts (al&eacute;atoires) et en extraire des pr&eacute;ceptes &eacute;l&eacute;mentaires., StatLearn23 </a>			

							
					</ol>
					
					<!-- <footer class="major">
						<ul class="actions">
							<li><a href="generic.html" class="button special">Get Started</a></li>
							<li><a href="generic.html" class="button">Learn More</a></li>
						</ul>
					</footer> -->
				</section>

				<section id="cta" class="main">
					<header class="major"> Contact </header>
					<ol>
							<li> Email: prenom.nom@po-ly-tech-ni-que.edu (without hyphens).</li>
							<li> Office 214, Tour 15-25, Jussieu Campus.</li>
					</ol>
				</section>
					
			</div>

			<!-- Footer -->
			<footer id="footer">
				
				
					<p class="copyright"><a href="https://en.wikipedia.org/wiki/Creative_Commons_license"><i class="fa fa-creative-commons"></i> BY-SA</a> Benjamin Guedj, 2016--2017. Design: <a href="https://html5up.net">HTML5 UP</a>.
						<br>&#8734;</p>
					</footer>

				</div>

				<!-- Scripts -->
				<script src="assets/js/jquery.min.js"></script>
				<script src="assets/js/jquery.scrollex.min.js"></script>
				<script src="assets/js/jquery.scrolly.min.js"></script>
				<script src="assets/js/skel.min.js"></script>
				<script src="assets/js/util.js"></script>
				<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
				<script src="assets/js/main.js"></script>

				<!-- Google Analytics -->
				<script>
					(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
						(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
						m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
					})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

					ga('create', 'UA-81430854-1', 'auto');
					ga('send', 'pageview');

				</script>

			</body>
			</html>
