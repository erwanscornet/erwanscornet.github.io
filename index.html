<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
	<title>Webpage of Erwan Scornet </title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<meta name="author" content="Erwan Scornet" />
	<meta name="description" content="Webpage of Erwan Scornet, assistant professor at Ecole Polytechnique" />
	<meta name="keywords" content="Erwan Scornet, Polytechnique, machine learning, statistique, random forests, forêts aléatoires, arbres" />
	<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
</head>
<body>

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header" class="alt">
			<!--<span class="logo"><img src="images/logo.svg" alt="" /></span>-->
			<h1>Erwan Scornet, Assistant professor at Ecole Polytechnique </h1>
			<h2>Statistic, Machine Learning </h2>

			<ul class="icons">
				<li>
					

</li>
<li><a href="https://github.com/erwanscornet" class="icon fa-github alt"><span class="label">GitHub</span></a></li>
<li><a href="https://scholar.google.fr/citations?user=6Qt1NFoAAAAJ&hl=fr" class="icon fa-google alt"><span class="label">Google Scholar</span></a></li>
</ul>
</header>

<!-- Nav -->
<nav id="nav">
	<ul>
		<li><a href="#header">Top</a></li>
		<li><a href="#intro" class="active">Bio</a></li>
		<!--<li><a href="#first">News</a></li>-->
		<!--<li><a href="#second">Research</a></li>-->
		<li><a href="#cta">Publications</a></li>
		<li><a href="#footer">Contact</a></li>
	</ul>
</nav>

<!-- Main -->
<div id="main">

	<!-- Introduction -->
	<section id="intro" class="main">
		<div class="spotlight">
			<div class="content">
				<header class="major">
					<h2>Short Bio</h2>
				</header>
				<div align="justify">
				<p>Since september 2016, I am an assistant professor at the Center for Applied Mathematics (CMAP) in Ecole Polytechnique near Paris. My research interests focus on theoretical statistics and Machine Learning with a particular emphasis on nonparametric estimates. I did my PhD thesis on a particular algorithm of Machine Learning called random forests, under the supervision of <a href="http://www.lsta.upmc.fr/biau.html"> G&eacuterard Biau </a> (LSTA - Paris 6) and <a href="http://cbio.ensmp.fr/~jvert/">  Jean-Philipe Vert </a> (Institut Curie). </p>
				</div> <!-- TODO -->
						<!-- <ul class="actions">
							<li><a href="" class="button">Curriculum Vit&aelig; (english)</a></li>
							<li><a href="" class="button">Curriculum Vit&aelig; (français)</a></li>
						</ul> -->
					</div>
					<span class="image"><img src="images/photo3.png" alt="" /></span>
				</div>
			<h2> <a href="pdf/cv_scornet_compressed.pdf">  Curriculum Vitae  </a> </h2>
			</section>

			<!-- First Section -->
			<section id="first" class="main special">
				<header class="major">
				<h2> Graduate Degree "Artificial Intelligence and Advanced Visual Computing" </h2>
				</header>

				<ul>
				A new graduate degree on Artificial Intelligence opened in September 2018 at Ecole Polytechnique.
				The official training website is <a href="https://programmes.polytechnique.edu/master-programmes/master-artificial-intelligence-advanced-visual-computing/descriptif-des-cours"> here </a> 
				and additional information on the scientific 
				content can be found <a href="http://www.lix.polytechnique.fr/Labo/Marie-Paule.Cani/MasterAI/doku.php?id=curriculum"> here </a>. 
				A short summary can also be found <a href="MasterAi-ViC-Mar2018.pdf"> here </a> (presentation of March 2019)
			</ul>

			<ul>
				If you are interested, <a href="https://portail.polytechnique.edu/graduatedegree/master/apply"> applications are here</a>. 
			</ul>
			</section>
			
			


			<!-- Third Section -->
			<section id="first" class="main special">
				<header class="major">
				<h2> Awards and distinctions </h2>
				</header>

				<ul>
				<li> Winner of the <a href="http://smai.emath.fr/spip.php?article359"> Jacques Neveu 2016 Prize </a>  for a thesis in the field of probability or statistic. 
				</ul>
			</section>

				<!--<header class="major">
					<h2>News</h2>
				</header>
				<ul class="features">-->
					<!-- <li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>Date</h3>
						<p><a href="#cta">Revised version of our preprint with </a> with <a href="http://math.univ-lille1.fr/~celisse/">Alain Celisse</a>, about stability and generalisation bounds for the Leave-one-Out.</p>
					</li> -->
					<!-- <li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>Date</h3>
						<p><a href="#cta">New preprint</a> with <a href="http://alquier.ensae.net">Pierre Alquier</a>, about xxx.</p>
					</li> -->
					
					<!--<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>December 9, 2017</h3>
						<p>I am running the <a href="http://nips.cc">NIPS 2017</a> workshop <em><a href="nips2017/50shadesbayesian.html">(Almost) 50 shades of Bayesian Learning: PAC-Bayesian trends and insights</a></em>.
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>November 8, 2017</h3>
						<p>Our paper with <a href="http://alquier.ensae.net">Pierre Alquier</a> on PAC-Bayesian bounds for hostile data has been accepted for publication in the <a href="http://www.springer.com/computer/ai/journal/10994">Machine Learning Journal</a>!
						</p>
					</li>
					<li>
						<span class="icon major style4 fa-graduation-cap"></span>
						<h3>November 1, 2017</h3>
						<p>Delighted to greet my new colleague and office mate <a href="http://chercheurs.lille.inria.fr/pgermain/">Pascal Germain</a>. Welcome! <a href="https://modal.lille.inria.fr/wikimodal/doku.php">Modal</a> is doubling its PAC-Bayes coverage!</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>October 28, 2017</h3>
						<p>Our paper with <a href="https://sites.google.com/site/sylvainrobbiano/">S. Robbiano</a> on PAC-Bayesian bounds for high-dimensional ranking has been accepted for publication in the <a href="https://www.journals.elsevier.com/journal-of-statistical-planning-and-inference/">Journal of Statistical Planning and Inference</a>!
						</p>
					</li>
					<li>
						<span class="icon major style4 fa-graduation-cap"></span>
						<h3>October 1, 2017</h3>
						<p>My new student Arthur Leroy has started his PhD. Welcome!</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>September 11, 2017</h3>
						<p>Our NIPS workshop <em><a href="nips2017/50shadesbayesian.html">(Almost) 50 shades of Bayesian Learning: PAC-Bayesian trends and insights</a></em> has been accepted by NIPS!
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>July 27, 2017</h3>
						<p>New version of our preprint and Python library <a href="https://github.com/bhargavvader/pycobra">pycobra</a> with Bhargav Srinivasa Desikan.
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>May 26, 2017</h3>
						<p>Our paper on quasi-Bayesian NMF (with <a href="http://alquier.ensae.net">Pierre Alquier</a>) is now published in <a href="http://www.springer.com/statistics/statistical+theory+and+methods/journal/12004"><em>Mathematical Methods of Statistics</em></a>!
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>April 26, 2017</h3>
						<p><a href="#cta">New preprint</a> with Bhargav Srinivasa Desikan on <a href="https://github.com/bhargavvader/pycobra">pycobra</a>, a Python library for ensemble regression analysis and visualisation.</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>April 7, 2017</h3>
						<p><a href="#cta">Revised version of our preprint</a> with Le Li and <a href="http://www.math.univ-angers.fr/~loustau/">Sébastien Loustau</a>, about quasi-Bayesian online clustering.</p>
					</li>
					<li>
						<span class="icon major style2 fa-code-fork"></span>
						<h3>March 27, 2017</h3>
						<p>New Python library released with Bhargav Srinivasa Desikan: <a href="https://github.com/bhargavvader/pycobra">pycobra</a> for ensemble regression analysis and visualisation.
						</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>February 1, 2017</h3>
						<p>Our paper on quasi-Bayesian NMF (with <a href="http://alquier.ensae.net">Pierre Alquier</a>) has been accepted for publication in <a href="http://www.springer.com/statistics/statistical+theory+and+methods/journal/12004"><em>Mathematical Methods of Statistics</em></a>!
						</p>
					</li>
					<li>
						<span class="icon major style4 fa-chevron-up"></span>
						<h3>January 1, 2017</h3>
						<p>I got promoted to a first class researcher (CR1) Inria position!</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>October 23, 2016</h3>
						<p><a href="#cta">New preprint</a> together with <a href="http://alquier.ensae.net">Pierre Alquier</a>, about PAC-Bayesian bounds for hostile data.</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>August 24, 2016</h3>
						<p><a href="#cta">Revised version of our preprint</a> with <a href="http://alquier.ensae.net">Pierre Alquier</a>, about quasi-Bayesian non-negative matrix factorization.</p>
					</li>
					<li>
						<span class="icon major style1 fa-files-o"></span>
						<h3>August 23, 2016</h3>
						<p><a href="#cta">New preprint</a> together with <a href="http://math.univ-lille1.fr/~celisse/">Alain Celisse</a>, about stability and generalisation bounds for the Leave-one-Out.</p>
					</li>
					<li>
						<span class="icon major style4 fa-graduation-cap"></span>
						<h3>August 22, 2016</h3>
						<p>My new student Bhargav Srinivasa Desikan has joined Inria. Welcome!</p>
					</li>
					<li>
						<span class="icon major style6 fa-cloud-upload"></span>
						<h3>July 27, 2016</h3>
						<p>New website!</p>
					</li>-->
					<!-- <li>
						<span class="icon major style1 fa-code"></span>
						<h3>Ipsum consequat</h3>
						<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
					</li>
					<li>
						<span class="icon major style3 fa-copy"></span>
						<h3>Amed sed feugiat</h3>
						<p>Sed lorem amet ipsum dolor et amet nullam consequat a feugiat consequat tempus veroeros sed consequat.</p>
					</li> -->
				</ul>
				<!-- <footer class="major">
					<ul class="actions">
						<li><a href="generic.html" class="button">Learn More</a></li>
					</ul>
				</footer> -->
			</section>

			<!-- Second Section -->
			<section id="second" class="main">
				<!--<header class="major">
					<h2>Research</h2>
				</header>
				<p>My main line of research is in statistical machine learning. I am primarily interested in the design, analysis and implementation of statistical learning methods for high dimensional problems. My interests include (but are not limited to): PAC-Bayesian theory, sparsity and high-dimensional statistics, optimisation theory, statistical learning theory, non-negative matrix factorisation, aggregation of estimators and classifiers, MCMC algorithms, (un)supervised learning, online clustering, concentration inequalities...
				</p>
				<ul class="statistics">
					<li class="style1">
						<span class="icon fa-files-o"></span>
						<strong>11</strong> Research Articles <br> (7 published)
					</li>
					<li class="style3">
						<span class="icon fa-commenting-o"></span>
						<strong>48</strong> Talks since 2010
					</li>
					<li class="style4">
						<span class="icon fa-graduation-cap"></span>
						<strong>10</strong> Students <br> (2 Ph.D.)
					</li>
					<li class="style5">
						<span class="icon fa-building-o"></span>
						<strong>4</strong> Collaborations with companies
					</li>
				</ul>-->
				<h2>Students</h2>
				<ol>
					<li>
						<a href="http://www.cmapx.polytechnique.fr/~jaouad.mourtada/">Jaouad Mourtada</a> (2016-2020) <br>
						Ph.D. student co-supervised with <a href="http://www.cmap.polytechnique.fr/~gaiffas/">Stéphane Gaïffas</a> 
					</li>

					<li>
						<a href="https://nprost.github.io/"> Nicolas Prost </a>  (2018-2019) <br>
						Ph.D. student co-supervised with <a href="http://juliejosse.com/">Julie Josse</a> and
						<a href="http://gael-varoquaux.info/">Gaël Varoquaux </a> 
					</li>

					<li>
						Clément Bénard (2018-2021) <br>
						Ph.D. student co-supervised with <a href="http://www.lsta.upmc.fr/biau.html">Gérard Biau </a> and
						<a href="https://www.linkedin.com/in/s%C3%A9bastien-da-veiga-80791013a/?originalSubdomain=fr">Sébastien Da Veiga </a> 
					</li>

					<li>
						<a href="https://ludovic-arnould.github.io/"> Ludovic Arnould </a> (2020-) <br>
						Ph.D. student co-supervised with <a href="http://www.lpsm.paris/pageperso/boyer/">Claire Boyer </a>  
					</li>

					<li>
						<a href="https://benedictecolnet.github.io/"> Bénédicte Colnet </a> (2020-) <br>
						Ph.D. student co-supervised with <a href="http://juliejosse.com/">Julie Josse</a> and
						<a href="http://gael-varoquaux.info/">Gaël Varoquaux </a> 
					</li>

					<li>
						<a href="https://alexisayme.github.io/"> Alexis Ayme </a> (2021-) <br>
						Ph.D. student co-supervised with <a href="http://www.lpsm.paris/pageperso/boyer/">Claire Boyer </a> and <a href="http://www.cmap.polytechnique.fr/~aymeric.dieuleveut/">Aymeric Dieuleveut </a>   
					</li>
					
				</ol>
					<!-- <footer class="major">
						<ul class="actions">
							<li><a href="generic.html" class="button">Learn More</a></li>
						</ul>
					</footer> -->
				</section>

				<!-- Get Started -->
				<section id="cta" class="main">
					<header class="major">
						<h2>Publications</h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>
					<h3>Preprints</h3>
					<ol>
						<li> J. Josse, N. Prost, E. Scornet, G. Varoquaux. <a href="https://arxiv.org/pdf/1902.06931.pdf"> On the consistency of supervised learning with missing values </a>, 2019  </li>
						<li> L. Arnould, C. Boyer, E. Scornet. <a href="https://hal.archives-ouvertes.fr/hal-03560047v1" > Is interpolation benign for random forests? </a>, 2022. </li>
						<li> B. Colnet, J. Josse, G. Varoquaux, E. Scornet. <a href="https://arxiv.org/abs/2208.07614"> Reweighting the RCT for generalization: finite sample analysis and variable selection </a>, 2022. </li>
						<li> P. Lutz, L. Arnould, C. Boyer, E. Scornet. <a href="https://hal.archives-ouvertes.fr/hal-03792008/document"> Sparse tree-based initialization for neural networks </a>, 2022. </li>
					
						
					</ol>
					
					<h3>Accepted/Published papers</h3>
					<ol>
						<li> Scornet, E., Biau, G. and Vert, J.-P. (2015). <a href="pdf/article.pdf">Consistency of random forests,</a> The Annals of Statistics, Vol. 43, pp. 1716-1741
							(<a href="pdf/supplementary_file.pdf">Supplementary materials </a>).
						<li> Scornet, E. (2016). <a href="pdf/article_asymptotics_RF">On the asymptotics of random forests,</a> Journal of Multivariate Analysis, Vol. 146, pp. 72-83.
						<li> Scornet, E. (2016). <a href="pdf/articlekernel.pdf"> Random forests and kernel methods,</a> IEEE Transactions on Information Theory, Vol. 62, pp. 1485-1500.
						<li> Biau, G., Scornet, E. (2016). <a href="pdf/reviewforest.pdf"> A Random Forest Guided Tour,</a> TEST, Vol. 25, pp. 197-227. (<a href="paper/test_rejoinder.pdf"> Discussion </a>).
						<li> Scornet, E. (2016). <a href="pdf/article_MATAPLI.pdf"> Promenade en for&ecircts al&eacuteatoires,</a> MATAPLI, Vol. 111. 
						<li> E. Bernard, Y. Jiao, E. Scornet, V. Stoven, T. Walter and J.-P. Vert (2017) <a href="http://www.biorxiv.org/content/early/2017/08/01/171298"> Kernel multitask regression for toxicogenetics,</a> Molecular Informatics, Vol. 36. 
						<li> J. Mourtada, S. Ga&iumlffas, E. Scornet, (2017) <a href="pdf/article_mondrian.pdf"> Universal consistency and minimax rates for online Mondrian Forest,</a> NIPS 2017 (<a href="paper/article_mondrian_supplementary.pdf">Supplementary materials </a>). 
						<li> Scornet, E. (2017). <a href="https://www.esaim-proc.org/articles/proc/abs/2017/05/contents/contents.html"> Tuning parameters in random forests,</a> ESAIM Procs, Vol. 60 pp. 144-162.</li>
						<li> R. Duroux, E. Scornet (2018) <a href="https://hal.archives-ouvertes.fr/hal-01287521"> Impact of subsampling and tree depth on random
							forests</a>, ESAIM: Probability and Statistics, Vol. 22, pp. 96-128. </li>
						<li> G. Biau, E. Scornet, J. Welbl, (2018) <a href="https://arxiv.org/abs/1604.07143"> Neural Random Forests </a>, Sankhya A, pp. 1-40.  </li>
						<li> J. Mourtada, S. Ga&iumlffas, E. Scornet (2020) <a href="https://arxiv.org/abs/1803.05784"> Minimax optimal rates for Mondrian trees and forests </a>, The Annals of Statistics, 48(4), 2253-2276. </li>
						<li> M. Le Morvan, N. Prost, J. Josse, E. Scornet. & G. Varoquaux (2020)  <a href="https://arxiv.org/abs/2002.00658"> Linear predictor on linearly-generated data with missing values: non consistency and solutions </a>, AISTAT. </li>
						<!-- pdf/article_aistat_2020.pdf --> 
						<li> M. Le Morvan, J. Josse, T. Moreau, E. Scornet, G. Varoquaux (2020) <a href="https://arxiv.org/abs/2007.01627"> Neumann networks: differential programming for supervised learning with missing values </a>, NeurIPS (oral communication). </li> 
						<li> C. B&eacute;nard, G. Biau, S. Da Veiga, E. Scornet (2021) <a href="https://arxiv.org/abs/1908.06852"> SIRUS: Stable and Interpretable RUle Set for Classification </a>, Electronic Journal of Statistics, Vol. 15, pp. 427-505.  </li>
						<li> C. B&eacute;nard, G. Biau, S. Da Veiga, E. Scornet (2021). <a href="https://arxiv.org/abs/2004.14841"> Interpretable Random Forests via Rule Extraction </a>, AISTAT.  </li>
						<li> J. Mourtada, S. Ga&iumlffas, E. Scornet (2021). <a href="https://arxiv.org/abs/1906.10529"> AMF: Aggregated Mondrian Forests for Online Learning </a>, Journal of the Royal Statistical Society: Series B (Statistical Methodology), 83(3), 505-533. </li> 
						<li> L. Arnould, C. Boyer, E. Scornet (2021). <a href="https://arxiv.org/abs/2010.15690"> Analyzing the tree-layer structure of Deep Forests </a>, ICML.  </li>
						<li> M. Le Morvan, J. Josse, E. Scornet, G. Varoquaux (2021). <a href="https://hal.archives-ouvertes.fr/hal-03243931">  What's a good imputation to predict with missing values? </a>, NeurIPS. </li>
						<li> E. Scornet (2021). <a href="https://arxiv.org/abs/2001.04295"> Trees, forests, and impurity-based variable importance </a>, Annales de l’Institut Henri Poincaré </li>
						<li> C. B&eacute;nard, G. Biau, S. Da Veiga, E. Scornet (2022). <a href="https://arxiv.org/pdf/2105.11724.pdf">  SHAFF: Fast and consistent SHApley eFfect estimates via random Forests </a>, AISTAT. </li>
						<li> C. B&eacute;nard, S. Da Veiga, E. Scornet (2022). <a href="https://arxiv.org/abs/2102.13347"> MDA for random forests: inconsistency, and a practical
							solution via the Sobol-MDA </a>, Biometrika. </li>	
						<li> A. Ayme, C. Boyer, A. Dieuleveut, E. Scornet (2022). <a href="https://arxiv.org/abs/2202.01463"> Near-optimal rate of consistency for linear models with missing values </a>, ICML. </li>
						<li> B. Colnet, J. Josse, E. Scornet, G. Varoquaux (2022). <a href="https://arxiv.org/abs/2105.06435"> Generalizing a causal effect: sensitivity analysis and missing covariates </a>, accepted for publication in Journal of Causal Inference. </li>

					</ol>

					<h3>Book</h3>
					<ol>
						<li>  B. Iooss, R. Kenett, P. Secchi, B.M. Colosimo, F. Centofanti, C. Bénard, S. Da Veiga, E. Scornet, S. N. Wood, Y. Goude, M. Fasiolo  <a href="https://link.springer.com/book/10.1007/978-3-031-12402-0"> Interpretability for Industry 4.0 : Statistical and Machine Learning Approaches </a>, 
							Editors: A. Lepore, B. Palumbo, J.-M. Poggi, Springer 2022. </li>
					</ol>

					<h3>Academic publications</h3>
					<ol>
						 <li> PhD thesis <a href="pdf/these.pdf"> Learning with random forests</a>, defended on Monday, 30th November, 2015.   </li>
						 <li> HDR manuscript <a href="pdf/HDR_scornet.pdf"> Random forests, interpretability, neural networks and missing values</a>, defended on the 17th December, 2020.   </li>
					</ol>
					<!-- <footer class="major">
						<ul class="actions">
							<li><a href="generic.html" class="button special">Get Started</a></li>
							<li><a href="generic.html" class="button">Learn More</a></li>
						</ul>
					</footer> -->
				</section>


				<section id="cta" class="main">
					<header class="major">
						<h2>Teaching</h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>
					
					

					
					Deep Learning course (slides + videos)
					
					<ol>
						<li>  <a href="teaching/RegularNN.pdf"> Vintage Neural Networks - Part 1 - Slides</a>
							<ol>
								<li> <a href="https://youtu.be/D2ERI765nZE"> Vintage Neural Networks 1.1 </a>	
								<li> <a href="https://youtu.be/zOb9l9Sz0KQ"> Vintage Neural Networks 1.2 </a>	
								<li> <a href="https://youtu.be/3nzsHcmmbrQ"> Vintage Neural Networks 1.3 </a>	
								<li> <a href="https://youtu.be/er0djNWVXm4"> Vintage Neural Networks 1.4 </a>			
								<li> <a href="https://youtu.be/Loacx9GDqHc"> Vintage Neural Networks 1.5 </a>			
								<li> <a href="https://youtu.be/x6g3RWz_Sho"> Vintage Neural Networks 1.6 </a>			
								<li> <a href="https://youtu.be/QhfnDjlkLAE"> Vintage Neural Networks 1.7 </a>			
								<li> <a href="https://youtu.be/lPRJI-aXjNQ"> Vintage Neural Networks 1.8 </a>			
								<li> <a href="https://youtu.be/BV-f-E5dc-Q"> Vintage Neural Networks 1.9 </a>			
							</ol>
						<li>  <a href="teaching/RegularNN.pdf"> Vintage Neural Networks - Part 2 (same set of slides) </a>
							<ol>
								<li> <a href="https://youtu.be/RfTEvXconys"> Vintage Neural Networks 2.1 </a>	
								<li> <a href="https://youtu.be/NJqp3Yd53lg"> Vintage Neural Networks 2.2 </a>	
								<li> <a href="https://youtu.be/NLbt1ZwA02A"> Vintage Neural Networks 2.3 </a>	
								<li> <a href="https://youtu.be/FPGGioKXDfE"> Vintage Neural Networks 2.4 </a>			
								<li> <a href="https://youtu.be/l-kg9cNkcB8"> Vintage Neural Networks 2.5 </a>			
								<li> <a href="https://youtu.be/xujC89lWKiE"> Vintage Neural Networks 2.6 </a>			
								<li> <a href="https://youtu.be/Xa5kG4Phqnk"> Vintage Neural Networks 2.7 </a>			
								<li> <a href="https://youtu.be/G0fkvsNa9Vk"> Vintage Neural Networks 2.8 </a>			
								<li> <a href="https://youtu.be/6AnCNa2djnU"> Vintage Neural Networks 2.9 </a>		
								<li> <a href="https://youtu.be/ND6b4_O1Uz0"> Vintage Neural Networks 2.10 </a>			
								<li> <a href="https://youtu.be/sMvii4ewa0k"> Vintage Neural Networks 2.11 </a>			
								<li> <a href="https://youtu.be/y6lzRdcl9aA"> Vintage Neural Networks 2.12 </a>			
								<li> <a href="https://youtu.be/w6bht7mDq1c"> Vintage Neural Networks 2.13 </a>			
								<li> <a href="https://youtu.be/Utc7mMQK9yU"> Vintage Neural Networks 2.14 </a>			
							</ol>
						<li> <a href="teaching/Optimization.pdf"> Optimization </a>
						<li>  <a href="teaching/CNN.pdf"> Convolutional Neural Networks </a>
							<ol>
								<li> <a href="https://youtu.be/3zC1UXk_aAM"> Convolutional Neural Networks 3.1 </a>	
								<li> <a href="https://youtu.be/Zz_kM0FpbbI"> Convolutional Neural Networks 3.2 </a>	
								<li> <a href="https://youtu.be/epk8y_DPoos"> Convolutional Neural Networks 3.3 </a>	
								<li> <a href="https://youtu.be/rvOWgrAH5qI"> Convolutional Neural Networks 3.4 </a>			
								<li> <a href="https://youtu.be/Jegh1HNWiQs"> Convolutional Neural Networks 3.5 </a>			
								<li> <a href="https://youtu.be/vWLpCE03__U"> Convolutional Neural Networks 3.6 </a>			
								<li> <a href="https://youtu.be/tJZVJ5d_JDM"> Convolutional Neural Networks 3.7 </a>			
								<li> <a href="https://youtu.be/i3b75ckAQ4s"> Convolutional Neural Networks 3.8 </a>			
								<li> <a href="https://youtu.be/tE3zZYRzTC4"> Convolutional Neural Networks 3.9 </a>		
								<li> <a href="https://youtu.be/KherWPJCpf8"> Convolutional Neural Networks 3.10 </a>			
								<li> <a href="https://youtu.be/EDQ_12IS30s"> Convolutional Neural Networks 3.11 </a>			
								<li> <a href="https://youtu.be/OcV3Mw6UgSg"> Convolutional Neural Networks 3.12 </a>			
								<li> <a href="https://youtu.be/_DbXS3UXhbo"> Convolutional Neural Networks 3.13 </a>			
								<li> <a href="https://youtu.be/QSnbg4RkBho"> Convolutional Neural Networks 3.14 </a>			
								<li> <a href="https://youtu.be/BS_jzavsF2I"> Convolutional Neural Networks 3.15 </a>			
								<li> <a href="https://youtu.be/yKkx5y4eITw"> Convolutional Neural Networks 3.16 </a>
								<li> <a href="https://youtu.be/lH2sd4hodDQ"> Convolutional Neural Networks 3.17 </a>									
								<li> <a href="https://youtu.be/NUeq3iZgFqQ"> Convolutional Neural Networks 3.18 </a>
									</ol>
						<li>  <a href="teaching/RNN.pdf"> Recurrent Neural Networks </a>
						<li>  <a href="teaching/VAE_GAN.pdf"> Generative Modelling </a>
						<li>  <a href="teaching/Word_Embedding.pdf"> Word Embedding </a> (in construction)
					</ol>
					
				
				</section>

				<section id="cta" class="main">
					<header class="major">
						<h2>Statistics and Video games - Stone's theorem </h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>

					<ol> You need to download the corresponding game package (Mac or PC) and launch the .exe file (you may need to download <a href="https://www.renpy.org/"> RenPy </a>).
					<li>  <a href="Theoreme_Stone.pdf"> Stone's theorem (statement) </a>
					<li>  <a href="Theoreme_Stone-1.0-mac.zip"> Game package for Mac</a>
					<li>  <a href="Theoreme_Stone-1.0-pc.zip"> Game package for PC</a>
						<ol>

				</section>

				<section id="cta" class="main">
					<header class="major">
						<h2>Talks</h2>
						<!-- &bull; <a href="">HALtools</a> -->
					</header>
					
					
					<ol>
						<li>  <a href="talks/random_forests.pdf"> Random Forests </a>
						<li> <a href="talks/Intro_to_AI.pdf"> General overview of AI </a>	
						<li> <a href="talks/AI_for_health.pdf"> Ai for health </a>	
						<li> <a href="talks/mdi.pdf"> Trees, forests, and impurity-based variable importance </a>			
					</ol>
					
					<!-- <footer class="major">
						<ul class="actions">
							<li><a href="generic.html" class="button special">Get Started</a></li>
							<li><a href="generic.html" class="button">Learn More</a></li>
						</ul>
					</footer> -->
				</section>

				<section id="cta" class="main">
					<header class="major"> Contact </header>
					<ol>
							<li> Email: prenom.nom@po-ly-tech-ni-que.edu (without hyphens).</li>
							<li> Office: 136, Turing Building, Route de Saclay, Palaiseau.</li>
							<li> Phone number: +33 1 77 57 80 80</li>
					</ol>
				</section>
					
			</div>

			<!-- Footer -->
			<footer id="footer">
				
				
					<p class="copyright"><a href="https://en.wikipedia.org/wiki/Creative_Commons_license"><i class="fa fa-creative-commons"></i> BY-SA</a> Benjamin Guedj, 2016--2017. Design: <a href="https://html5up.net">HTML5 UP</a>.
						<br>&#8734;</p>
					</footer>

				</div>

				<!-- Scripts -->
				<script src="assets/js/jquery.min.js"></script>
				<script src="assets/js/jquery.scrollex.min.js"></script>
				<script src="assets/js/jquery.scrolly.min.js"></script>
				<script src="assets/js/skel.min.js"></script>
				<script src="assets/js/util.js"></script>
				<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
				<script src="assets/js/main.js"></script>

				<!-- Google Analytics -->
				<script>
					(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
						(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
						m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
					})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

					ga('create', 'UA-81430854-1', 'auto');
					ga('send', 'pageview');

				</script>

			</body>
			</html>
